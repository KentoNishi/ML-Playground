{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Dot, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from keras.layers import dot\n",
    "import urllib\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(filename, url, expected_bytes):\n",
    "    if not os.path.exists(os.path.join(os.path.join(os.path.abspath(''),\"data\"),filename)):\n",
    "        os.mkdir(os.path.join(os.path.abspath(''),\"data\"))\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, os.path.join(os.path.join(os.path.abspath(''),\"data\"),filename))\n",
    "    statinfo = os.stat(os.path.join(os.path.join(os.path.abspath(''),\"data\"),filename))\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return os.path.join(os.path.join(os.path.abspath(''),\"data\"),filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(vocabulary_size=10000):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = download('text8.zip', url, 31344016)\n",
    "    vocabulary = read_data(filename)\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,vocabulary_size)\n",
    "    del vocabulary\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified C:\\Users\\yoshi\\Documents\\GitHub\\ML-Playground\\word2vec-experiments\\data\\text8.zip\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0928 16:53:55.051672 20372 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0928 16:53:55.260038 20372 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_target = Input((1,))\n",
    "input_context = Input((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 16:54:03.050639 20372 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "similarity = dot([target,context],axes=1,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  import sys\n",
      "W0928 16:54:26.935605 20372 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0928 16:54:26.955589 20372 deprecation_wrapper.py:119] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0928 16:54:26.963580 20372 deprecation.py:323] From c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = dot([target, context], normalize=False, axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# create the primary training model\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yoshi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"do...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(input=[input_target, input_context], output=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self,word):\n",
    "        valid_word = reverse_dictionary[dictionary[word]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        sim = self._get_sim(dictionary[word])\n",
    "        nearest = (-sim).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "            close_word = reverse_dictionary[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.6809330582618713\n",
      "Iteration 1000, loss=0.7051646113395691\n",
      "Iteration 2000, loss=0.6949292421340942\n",
      "Iteration 3000, loss=1.1329927444458008\n",
      "Iteration 4000, loss=0.7127837538719177\n",
      "Iteration 5000, loss=0.7452524304389954\n",
      "Iteration 6000, loss=0.6771693825721741\n",
      "Iteration 7000, loss=0.6588912606239319\n",
      "Iteration 8000, loss=0.6899852752685547\n",
      "Iteration 9000, loss=0.7537251114845276\n",
      "Iteration 10000, loss=0.6427937746047974\n",
      "Iteration 11000, loss=0.6095649003982544\n",
      "Iteration 12000, loss=0.7932949662208557\n",
      "Iteration 13000, loss=0.6741223335266113\n",
      "Iteration 14000, loss=0.6151401400566101\n",
      "Iteration 15000, loss=0.6587762236595154\n",
      "Iteration 16000, loss=0.8415100574493408\n",
      "Iteration 17000, loss=0.7330864667892456\n",
      "Iteration 18000, loss=0.5995334386825562\n",
      "Iteration 19000, loss=0.7023196220397949\n",
      "Iteration 20000, loss=0.7869294285774231\n",
      "Iteration 21000, loss=0.4649149179458618\n",
      "Iteration 22000, loss=0.6846593618392944\n",
      "Iteration 23000, loss=0.6644384264945984\n",
      "Iteration 24000, loss=0.6148537993431091\n",
      "Iteration 25000, loss=0.6432870030403137\n",
      "Iteration 26000, loss=0.5925767421722412\n",
      "Iteration 27000, loss=0.6040145754814148\n",
      "Iteration 28000, loss=0.6025989651679993\n",
      "Iteration 29000, loss=0.9018740057945251\n",
      "Iteration 30000, loss=0.6929477453231812\n",
      "Iteration 31000, loss=0.2783833146095276\n",
      "Iteration 32000, loss=0.5343527793884277\n",
      "Iteration 33000, loss=0.6242730617523193\n",
      "Iteration 34000, loss=0.6921140551567078\n",
      "Iteration 35000, loss=0.7483494281768799\n",
      "Iteration 36000, loss=0.10318080335855484\n",
      "Iteration 37000, loss=0.8755600452423096\n",
      "Iteration 38000, loss=0.5727262496948242\n",
      "Iteration 39000, loss=0.6341468691825867\n",
      "Iteration 40000, loss=0.8149736523628235\n",
      "Iteration 41000, loss=0.6791034936904907\n",
      "Iteration 42000, loss=0.6301568746566772\n",
      "Iteration 43000, loss=0.1577424556016922\n",
      "Iteration 44000, loss=0.7000642418861389\n",
      "Iteration 45000, loss=0.5777212977409363\n",
      "Iteration 46000, loss=0.8033852577209473\n",
      "Iteration 47000, loss=0.7726189494132996\n",
      "Iteration 48000, loss=0.6526132822036743\n",
      "Iteration 49000, loss=1.2437496185302734\n",
      "Iteration 50000, loss=0.613849401473999\n",
      "Iteration 51000, loss=0.5812864303588867\n",
      "Iteration 52000, loss=0.6297076940536499\n",
      "Iteration 53000, loss=0.585395097732544\n",
      "Iteration 54000, loss=0.6686281561851501\n",
      "Iteration 55000, loss=0.010725712403655052\n",
      "Iteration 56000, loss=0.644414484500885\n",
      "Iteration 57000, loss=0.5904954671859741\n",
      "Iteration 58000, loss=0.4820879101753235\n",
      "Iteration 59000, loss=0.632893979549408\n",
      "Iteration 60000, loss=0.06538625061511993\n",
      "Iteration 61000, loss=1.042371153831482\n",
      "Iteration 62000, loss=0.480846643447876\n",
      "Iteration 63000, loss=1.124695062637329\n",
      "Iteration 64000, loss=0.5029691457748413\n",
      "Iteration 65000, loss=0.568101704120636\n",
      "Iteration 66000, loss=0.49299857020378113\n",
      "Iteration 67000, loss=0.5881340503692627\n",
      "Iteration 68000, loss=0.4913053512573242\n",
      "Iteration 69000, loss=0.5476192235946655\n",
      "Iteration 70000, loss=0.9333085417747498\n",
      "Iteration 71000, loss=0.9063394665718079\n",
      "Iteration 72000, loss=0.4934118092060089\n",
      "Iteration 73000, loss=0.00029717342113144696\n",
      "Iteration 74000, loss=0.7730827927589417\n",
      "Iteration 75000, loss=0.5207424759864807\n",
      "Iteration 76000, loss=0.10025015473365784\n",
      "Iteration 77000, loss=0.5971910357475281\n",
      "Iteration 78000, loss=0.8266316056251526\n",
      "Iteration 79000, loss=0.7988917827606201\n",
      "Iteration 80000, loss=0.5653464794158936\n",
      "Iteration 81000, loss=0.40089327096939087\n",
      "Iteration 82000, loss=0.5961837768554688\n",
      "Iteration 83000, loss=0.4383363425731659\n",
      "Iteration 84000, loss=1.078403115272522\n",
      "Iteration 85000, loss=0.47581803798675537\n",
      "Iteration 86000, loss=1.0854040384292603\n",
      "Iteration 87000, loss=0.6723084449768066\n",
      "Iteration 88000, loss=0.7328312397003174\n",
      "Iteration 89000, loss=0.9942044019699097\n",
      "Iteration 90000, loss=0.5983891487121582\n",
      "Iteration 91000, loss=1.1528400182724\n",
      "Iteration 92000, loss=0.3604593873023987\n",
      "Iteration 93000, loss=0.00151415029540658\n",
      "Iteration 94000, loss=0.5198245048522949\n",
      "Iteration 95000, loss=0.6216484308242798\n",
      "Iteration 96000, loss=0.40610799193382263\n",
      "Iteration 97000, loss=0.4102346897125244\n",
      "Iteration 98000, loss=0.1729433536529541\n",
      "Iteration 99000, loss=0.4132375419139862\n",
      "Iteration 100000, loss=0.40443646907806396\n",
      "Iteration 101000, loss=0.5213322043418884\n",
      "Iteration 102000, loss=0.06934404373168945\n",
      "Iteration 103000, loss=0.7811732292175293\n",
      "Iteration 104000, loss=0.10548319667577744\n",
      "Iteration 105000, loss=3.8125178813934326\n",
      "Iteration 106000, loss=0.8682849407196045\n",
      "Iteration 107000, loss=2.193475120293442e-05\n",
      "Iteration 108000, loss=0.35404881834983826\n",
      "Iteration 109000, loss=0.004473424982279539\n",
      "Iteration 110000, loss=0.02831779234111309\n",
      "Iteration 111000, loss=1.0326517820358276\n",
      "Iteration 112000, loss=0.39326751232147217\n",
      "Iteration 113000, loss=0.42244306206703186\n",
      "Iteration 114000, loss=0.05433180183172226\n",
      "Iteration 115000, loss=0.5275906324386597\n",
      "Iteration 116000, loss=0.6594703793525696\n",
      "Iteration 117000, loss=0.35267066955566406\n",
      "Iteration 118000, loss=0.12380026280879974\n",
      "Iteration 119000, loss=0.4798671007156372\n",
      "Iteration 120000, loss=7.629427273059264e-06\n",
      "Iteration 121000, loss=0.34548473358154297\n",
      "Iteration 122000, loss=0.2063920646905899\n",
      "Iteration 123000, loss=0.3885332942008972\n",
      "Iteration 124000, loss=1.4991481304168701\n",
      "Iteration 125000, loss=0.3773231506347656\n",
      "Iteration 126000, loss=0.34612953662872314\n",
      "Iteration 127000, loss=0.33083659410476685\n",
      "Iteration 128000, loss=0.4902598261833191\n",
      "Iteration 129000, loss=0.403995543718338\n",
      "Iteration 130000, loss=0.2879372239112854\n",
      "Iteration 131000, loss=0.5254392623901367\n",
      "Iteration 132000, loss=0.6220598816871643\n",
      "Iteration 133000, loss=0.8755655884742737\n",
      "Iteration 134000, loss=0.4621325135231018\n",
      "Iteration 135000, loss=0.43518564105033875\n",
      "Iteration 136000, loss=0.46158942580223083\n",
      "Iteration 137000, loss=0.47624140977859497\n",
      "Iteration 138000, loss=0.31484952569007874\n",
      "Iteration 139000, loss=0.6828108429908752\n",
      "Iteration 140000, loss=1.0795425176620483\n",
      "Iteration 141000, loss=0.014327029697597027\n",
      "Iteration 142000, loss=0.6825835108757019\n",
      "Iteration 143000, loss=1.192093321833454e-07\n",
      "Iteration 144000, loss=1.216200351715088\n",
      "Iteration 145000, loss=0.0013899921905249357\n",
      "Iteration 146000, loss=0.054016683250665665\n",
      "Iteration 147000, loss=0.6286159157752991\n",
      "Iteration 148000, loss=0.0003365843731444329\n",
      "Iteration 149000, loss=0.5659352540969849\n",
      "Iteration 150000, loss=0.32436561584472656\n",
      "Iteration 151000, loss=1.2794816493988037\n",
      "Iteration 152000, loss=0.3759923577308655\n",
      "Iteration 153000, loss=0.3822992444038391\n",
      "Iteration 154000, loss=1.192093321833454e-07\n",
      "Iteration 155000, loss=0.393099844455719\n",
      "Iteration 156000, loss=0.371418833732605\n",
      "Iteration 157000, loss=1.1951217651367188\n",
      "Iteration 158000, loss=0.4456484317779541\n",
      "Iteration 159000, loss=0.8060479164123535\n",
      "Iteration 160000, loss=1.6452569961547852\n",
      "Iteration 161000, loss=0.4430133104324341\n",
      "Iteration 162000, loss=1.192093321833454e-07\n",
      "Iteration 163000, loss=0.008608391508460045\n",
      "Iteration 164000, loss=0.31344074010849\n",
      "Iteration 165000, loss=0.1530846655368805\n",
      "Iteration 166000, loss=0.14932991564273834\n",
      "Iteration 167000, loss=0.4735005497932434\n",
      "Iteration 168000, loss=0.33090832829475403\n",
      "Iteration 169000, loss=0.39314618706703186\n",
      "Iteration 170000, loss=0.8305221796035767\n",
      "Iteration 171000, loss=0.5701234936714172\n",
      "Iteration 172000, loss=1.118071436882019\n",
      "Iteration 173000, loss=0.4111995995044708\n",
      "Iteration 174000, loss=0.3813268542289734\n",
      "Iteration 175000, loss=0.813046395778656\n",
      "Iteration 176000, loss=0.31926998496055603\n",
      "Iteration 177000, loss=1.1643208265304565\n",
      "Iteration 178000, loss=0.3518575131893158\n",
      "Iteration 179000, loss=0.0008490113541483879\n",
      "Iteration 180000, loss=0.3702999949455261\n",
      "Iteration 181000, loss=0.3316098749637604\n",
      "Iteration 182000, loss=0.4365311861038208\n",
      "Iteration 183000, loss=0.4944179058074951\n",
      "Iteration 184000, loss=1.1370906829833984\n",
      "Iteration 185000, loss=0.018191896378993988\n",
      "Iteration 186000, loss=0.821044385433197\n",
      "Iteration 187000, loss=1.030947208404541\n",
      "Iteration 188000, loss=1.0442862510681152\n",
      "Iteration 189000, loss=0.36596977710723877\n",
      "Iteration 190000, loss=0.3103930950164795\n",
      "Iteration 191000, loss=0.8944963216781616\n",
      "Iteration 192000, loss=0.39815184473991394\n",
      "Iteration 193000, loss=0.8283431529998779\n",
      "Iteration 194000, loss=0.7929118275642395\n",
      "Iteration 195000, loss=1.4180551767349243\n",
      "Iteration 196000, loss=0.23918062448501587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 197000, loss=0.33350422978401184\n",
      "Iteration 198000, loss=1.2639933824539185\n",
      "Iteration 199000, loss=1.2330348491668701\n"
     ]
    }
   ],
   "source": [
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 1000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_model.save(\"validation_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"classification_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
